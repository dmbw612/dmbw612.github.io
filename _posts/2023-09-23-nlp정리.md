---
layout: single
title:  "NLP 전처리 정리를!"

---


• Corpus 문서전체
• Token 구성 단위
 - Work Tkn.
 - Sent. Tkn.
• 어절: 띄어쓰기로 나눈 단위
• 형태소: 뜻을 가진 최소 단위
• 토큰화 방법
 - 영어는 어절~단어~의미라서 Word TKN.
 - 한국어는 형태소~의미라서 형태소 TKN.


1. 내가 가진 전체 texts: corpus (C)
2. C를 S의 index로 구성된 matrix로 변환 
   - C를 tokenization해서 단어 집합 S를 구성
     - 토큰화 방법 결정 (e.g., 단어토큰화, 단어구두점토큰화, 형태소토큰화, Penn Treebank Tokenization, SOYNLP)

           from nltk.tokenize import word_tokenize
           from nltk.tokenize import WordPunctTokenizer
           from tensorflow.keras.preprocessing.text import text_to_word_sequence
           from soynlp.tokenizer import MaxScoreTokenizer

   - 형태소 분석기 결정 (e.g., Okt, 메캅, 코모라, 한나눔 등 in KoNLPy)
   - 전처리 방법 결정(cleaning/normalization/lemmatization/stopwords 등)
   - 단어집합 S 구성 (S는 C를 전처리를 거친 {word:index} sets), param:maxwords
   - Param(Maxlen/ padding) 결정해서 C를 matrix로 변환
 - 문장 c의 차원은 1 x maxlen인데, 이를 1 x (maxlen, n_dim)의 embedding으로 변환

C ~ f(|S|, (max_len, n_dim))

1. 1개의 문장을 (1, |S|) 차원으로 변환. (e.g., [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, ..., 1, 0, 0, 0])
2. 1개의 문장을 (1, max_len, n_dim) 차원으로 embedding.
3. 1개의 문장을 (1, max_len)으로 변환 (e.g, [399, 1045, 23, 8890, 5050, 3, 1])
