---
layout: single
title:  "NLP 전처리 정리를!"

---
[INPUT에 대하여]
우리가 처리하고자 하는 문서들 전체를 Corpus (C)라고 하자.
Corpus로 부터 단어 집합 (S)를 구성한다. (parameter로 |S|를 의미하는 max_words가 필요하다.) 단어집합에 들어가지 않는 단어들은 무시한다.
단어 집합을 구성하기 위해서는 먼저 token에 대해서 정의해야 하는데, token에 대해서 (1)어절, (2)형태소 기준으로 token을 정의해 볼 수 있다.
예시 text "국제사회와 우리의 노력들로 범죄를 척결하자"에 대해서 tokenization을 진행한 예는 아래와 같다.
  (1) 어절 기준으로 token화하면  → ['국제사회와', '우리의', '노력들로', '범죄'를', '척결하자']
  (2) 형태소 기준으로 token화하면 → ['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']
  (3) 단어 기준으로 token화도 하는지(?) → ['국제사회', '우리', '노력', '범죄', '척결하다']
(보통 unknown, padding, <sos>, <eos> token이 index 0,1,2,3에 기본으로 들어감)


     - 토큰화 방법 결정 (e.g., 단어토큰화, 단어구두점토큰화, 형태소토큰화, Penn Treebank Tokenization, SOYNLP)

           from nltk.tokenize import word_tokenize
           from nltk.tokenize import WordPunctTokenizer
           from tensorflow.keras.preprocessing.text import text_to_word_sequence
           from soynlp.tokenizer import MaxScoreTokenizer

C를 컴퓨터가 이해할 수 있는 숫자로 구성된 matrix C'로 변환한다. 다양한 차원의 C'으로 변환 가능하다.
   (1) N x |S|     (예시: [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, …, 1, 0, 0, 0])
   (2) N x max_len    (예시: [0, 0, 0, 399, 1045, 23, 8890, 550, 3, 1])
   (3) N x (max_len x n_dim)      (embedding layer에서 n_dim 결정해줘야함)

[기억해야 할 Parameters]
NLP param.
   - max_words: 하나의 Task에서 사용할 최대 단어 개수
   - max_len: 한 문장을 구성하는 최대 단어 개수
   - n_dim: 한 단어를 mapping 할 차원의 수
Network param.
   - n_hidden: network의 hidden unit 개수 

[MODEL에 대하여]
RNN은 입력되는 단어 개수만큼 반복되어 encoder가 출력하지만, 
Transformer에서는 입력 문장이 한번에 입력되어 1회 encoder가 작동하고, decoder에서 <eos>가 나올때까지 반복된다.

V개의 단어로 구성된, 그리고 d_model 차원으로 embedding된 한 문장 (V x d_model)을 d_head 차원의 저차원으로 다시 한 번 더 embedding하려고 하는데,
이 때 문맥을 고려해서 embedding을 하고 싶다 그래서 QK^T/root(d_head)라는 문맥을 알고 있는 attention energy(?)를 사용한다.
결국 원래의 문장은 문맥을 고려한 V x d_head 차원의 attention 문장으로 변경된다. 
이러한 attention_j 문장들이 h개 만큼 있으므로 concat(attention_1, ..., attention_h)을 수행해서 차원의 attention 문장(V x d_model)을 만들 수 있다.
이 attention 문장을 다히 W_o (d_model x d_head)와 행렬곱해주면 V x d_head 차원의 최종 output을 만들 수 있다. (다시 d_head차원으로 변경해야 하는 이유는 decoder에 각 head에 들어가야 하기 때문이다.)
이로서 encoder에서의 self-attetion 진행이 끝났다.

모델의 한 단어(x_i, (1 x d_model), i=1,...,V) → Q_ij/K_ii'j/V_ii'j (1 x d_head) # i'=1,...,V, j = 1,...,h
(d_head = d_model / h, V=max_len, d_model=n_dim)
attention energy_i,i' = Q_i (1 x d_head) X K_i'^T (d_head x 1) / root(d_head)
attention_i = softmax([ae_i'|i'=1,..,V]) * V_i' (1 x d_head = (1 x d_head) * (1 x d_head))
